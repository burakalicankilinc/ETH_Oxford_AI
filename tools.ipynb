{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: uv in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.9.16)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[2mUsing Python 3.13.2 environment at: /Library/Frameworks/Python.framework/Versions/3.13\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m10 packages\u001b[0m \u001b[2min 19ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
<<<<<<< Updated upstream
    "#install requirements\n",
    "%pip install uv\n",
    "!uv pip install --system langgraph langchain langchain-core langchain-openai langchain-valyu python-dotenv requests google-api-python-client google-auth-httplib2 google-auth-oauthlib\n"
=======
    "# Force reinstall all langchain packages to the latest matching versions\n",
    "%pip install -U --force-reinstall langchain langchain-community langchain-core langchain-google-genai valyu prophet yfinance matplotlib pandas"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import os\n",
    "import base64\n",
    "from typing import List, Optional\n",
    "import getpass\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "from langchain.agents import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from pathlib import Path\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"FILL\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"FILL\"\n",
    "\n",
    "model = init_chat_model(\"gpt-4.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trend tools\n",
    "@tool \n",
    "def arPersistance():\n",
    "    \"\"\" \n",
    "    Brownian model for stocks\n",
    "    \"\"\"\n",
    "\n",
    "@tool\n",
    "def mlModel():\n",
    "    \"\"\" \n",
    "    RNN model for stock prediction\n",
    "    \"\"\"\n",
    "\n",
    "#noise tools\n",
    "@tool \n",
    "def generalInfo():\n",
    "    \"\"\" \n",
    "    information regarding the general market of the stock\n",
    "    \"\"\"\n",
    "\n",
    "@tool \n",
    "def specificInfo():\n",
    "    \"\"\" \n",
    "    information regarding the stock\n",
    "    \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trend agent\n",
    "trendAgent = create_openai_functions_agent(\n",
    "    model,\n",
    "    tools=[arPersistance, mlModel],\n",
    "    prompt= FILL,\n",
    ")\n",
    "\n",
    "trendAgentExecuter = AgentExecutor(\n",
    "    agent=trendAgent, \n",
    "    tools=[arPersistance, mlModel], \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "#noise agent\n",
    "noiseAgent = create_openai_functions_agent(\n",
    "    model,\n",
    "    tools=[generalInfo, specificInfo],\n",
    "    prompt= FILL,\n",
    ")\n",
    "\n",
    "noiseAgentExecuter = AgentExecutor(\n",
    "    agent=noiseAgent, \n",
    "    tools=[generalInfo, specificInfo], \n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< Updated upstream
    "@tool \n",
    "def trendAgentTool():\n",
    "    trendAgentExecuter.invoke()\n",
=======
    "@tool\n",
    "def mlModel(ticker: str):\n",
    "    \"\"\" \n",
    "    Uses Facebook Prophet to predict stock prices 30 days ahead.\n",
    "    Returns daily price targets and saves a plot to a .png file.\n",
    "    the png files name is generated by graph_filename = f\"{ticker}_forecast.png\"\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # search stock data\n",
    "\n",
    "        data = yf.Ticker(ticker).history(period=\"2y\")\n",
    "        if data.empty:\n",
    "            return \"Error: No data found\"\n",
    "        \n",
    "        # Format for prophet\n",
    "        \n",
    "        df = data.reset_index()\n",
    "        df['ds'] = df['Date'].dt.tz_localize(None)\n",
    "        df['y'] = df['Close']\n",
    "\n",
    "        # Train\n",
    "\n",
    "        m = Prophet(daily_seasonality=True)\n",
    "        m.fit(df)\n",
    "\n",
    "        # Predict\n",
    "\n",
    "        future = m.make_future_dataframe(periods=30)\n",
    "        forecast = m.predict(future)\n",
    "\n",
    "        # Save the Graph\n",
    "\n",
    "        fig1 = m.plot(forecast)\n",
    "        plt.title(f\"{ticker} 30-Day Forecast\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Price\")\n",
    "\n",
    "        graph_filename = f\"{ticker}_forecast.png\"\n",
    "        plt.savefig(graph_filename)\n",
    "        plt.close()\n",
    "\n",
    "        # List 30 day prediction\n",
    "        future_data = forecast.tail(30)\n",
    "\n",
    "        daily_tracking = []\n",
    "        for index, row in future_data.iterrows():\n",
    "            date_str = row['ds'].strftime('%Y-%m-%d')\n",
    "            price_str = f\"${row['yhat']:.2f}\"\n",
    "            daily_tracking.append(f\"{date_str}: {price_str}\")\n",
    "\n",
    "        daily_summary = \"\\n\".join(daily_tracking)\n",
    "\n",
    "        latest_pred = forecast.iloc[-1]['yhat']\n",
    "        current_price = df.iloc[-1]['y']\n",
    "        trend = \"UP\" if latest_pred > current_price else \"DOWN\"\n",
    "        print(trend)\n",
    "        \n",
    "        # Return Everything\n",
    "        return (f\"Analysis Complete for {ticker}\\n\"\n",
    "                f\"Graph saved to: {graph_filename}\\n\"\n",
    "                f\"Trend: {trend}\\n\\n\"\n",
    "                f\"Daily Price Targets (Next 30 Days):\\n\"\n",
    "                f\"{daily_summary}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Prediction failed: {e}\"\n",
    "    \n",
>>>>>>> Stashed changes
    "\n",
    "@tool \n",
    "def noiseAgentTool():\n",
    "    noiseAgentExecuter.invoke()\n",
    "\n",
<<<<<<< Updated upstream
    "supAgent = create_openai_functions_agent(\n",
    "    model,\n",
    "    tools=[trendAgentTool, noiseAgentTool],\n",
    "    prompt= FILL,\n",
    ")"
=======
    "        end_date = pd.Timestamp.today().normalize()\n",
    "        start_date = end_date - pd.DateOffset(years=HISTORICAL_YEARS)\n",
    "        pred_end_date = end_date + pd.tseries.offsets.BDay(PREDICTION_DAYS)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Download and prepare data\n",
    "        # -----------------------------\n",
    "        # Fix for yfinance returning multi-index\n",
    "        prices = yf.download(tickers=stock_name, start=start_date, end=pred_end_date, progress=False)\n",
    "        \n",
    "        if prices.empty:\n",
    "            return f\"Error: No data found for {stock_name}\"\n",
    "\n",
    "        # Handle yfinance MultiIndex (recent API change)\n",
    "        if isinstance(prices.columns, pd.MultiIndex):\n",
    "            prices = prices['Close']\n",
    "            # If still a dataframe (multiple tickers?), select the specific ticker\n",
    "            if isinstance(prices, pd.DataFrame) and stock_name in prices.columns:\n",
    "                 prices = prices[stock_name]\n",
    "        elif 'Close' in prices.columns:\n",
    "            prices = prices['Close']\n",
    "        \n",
    "        # Ensure we have a Series, not a DataFrame\n",
    "        if isinstance(prices, pd.DataFrame):\n",
    "             prices = prices.iloc[:, 0]\n",
    "\n",
    "        # Generate business days (weekdays only)\n",
    "        future_dates = pd.bdate_range(start=pd.to_datetime(end_date) + pd.Timedelta(days=1),\n",
    "                        end=pd.to_datetime(pred_end_date))\n",
    "\n",
    "        train_set = prices.loc[:end_date]\n",
    "        \n",
    "        if len(train_set) < 2:\n",
    "            return \"Error: Not enough historical data for prediction.\"\n",
    "\n",
    "        daily_returns = ((train_set / train_set.shift(1)) - 1).dropna()\n",
    "\n",
    "        So = train_set.iloc[-1]\n",
    "        dt = 1  # day\n",
    "        \n",
    "        # Calculate volatility and drift\n",
    "        mu = np.mean(daily_returns)\n",
    "        sigma = np.std(daily_returns)\n",
    "        \n",
    "        # If sigma is 0 or NaN, we can't predict\n",
    "        if np.isnan(sigma) or sigma == 0:\n",
    "            return \"Error: Volatility calculation failed (sigma is 0 or NaN).\"\n",
    "\n",
    "        # Simulation\n",
    "        T_days = len(future_dates)\n",
    "        N = T_days\n",
    "        t = np.arange(1, N + 1)\n",
    "\n",
    "        b = {str(scen): np.random.normal(0, 1, N) for scen in range(1, scen_size + 1)}\n",
    "        W = {str(scen): b[str(scen)].cumsum() for scen in range(1, scen_size + 1)}\n",
    "\n",
    "        drift = (mu - 0.5 * sigma ** 2) * t\n",
    "        diffusion = {str(scen): sigma * W[str(scen)] for scen in range(1, scen_size + 1)}\n",
    "\n",
    "        S = np.array([So * np.exp(drift + diffusion[str(scen)]) for scen in range(1, scen_size + 1)])\n",
    "        \n",
    "        # Average prediction\n",
    "        S_pred = np.mean(S, axis=0)\n",
    "\n",
    "        final_df = pd.DataFrame({\n",
    "            'pred': S_pred\n",
    "        }, index=future_dates[:len(S_pred)])\n",
    "\n",
    "        # Create output string\n",
    "        rows = [f\"{date.date()}: ${price:.2f}\" for date, price in zip(final_df.index, final_df['pred'])]\n",
    "        result = '\\n'.join(rows)\n",
    "        print(result)\n",
    "\n",
    "        if not result:\n",
    "            return \"Error: Model ran but produced no output rows.\"\n",
    "\n",
    "        return f\"Brownian Motion Forecast for {stock_name}:\\n{result}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        # This prevents the 'ValueError: contents are required' crash\n",
    "        return f\"Brownian Model failed: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool \n",
    "#query_general is the prompt given to Valyu, and it should be on general / industry specific news/articles instead of stoc specific.\n",
    "#max_results refers to the number of sources that should be returned by the function. (Top N)\n",
    "def generalInfo(query_general: str) -> str:\n",
    "    \"\"\"\n",
    "    query_general: the query that is to be sent to the ai, to find information regarding the news and researches focused on the macroeconomy and broad industry-specific news and researches relating to the stock. The top 10 relevant information will be recorded. \n",
    "    news within the most recent 12 months will be considered.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the API key from environment\n",
    "    API_KEY = os.environ.get(\"VALYU_API_KEY\")\n",
    "    # Initialize the Valyu client\n",
    "    valyu = Valyu(api_key=API_KEY)\n",
    "\n",
    "    # ---------- NEWS SEARCH ----------\n",
    "    news_response = valyu.search(\n",
    "        query=query_general,\n",
    "        search_type=\"news\",\n",
    "        max_num_results=10,\n",
    "        relevance_threshold=0.7,\n",
    "        max_price=0.0, #free content only\n",
    "        start_date=datetime.now().date() - timedelta(days=365),\n",
    "        end_date=datetime.now().date(),\n",
    "        excluded_sources=[\"reddit.com\", \"twitter.com\", \"x.com\"],\n",
    "        response_length=\"medium\",\n",
    "        fast_mode=False,\n",
    "    )\n",
    "\n",
    "    # ---------- PROPRIETARY SEARCH ----------\n",
    "    proprietary_response = valyu.search(\n",
    "        query=query_general,\n",
    "        search_type=\"proprietary\",\n",
    "        max_num_results=10,\n",
    "        relevance_threshold=0.7,\n",
    "        max_price=0.0,\n",
    "        start_date=datetime.now().date() - timedelta(days=365),\n",
    "        end_date=datetime.now().date(),\n",
    "        response_length=\"medium\",\n",
    "        fast_mode=False,\n",
    "    )\n",
    "\n",
    "    # Combine results safely\n",
    "    response = (\n",
    "        (news_response.get(\"results\") or []) +\n",
    "        (proprietary_response.get(\"results\") or [])\n",
    "    )\n",
    "\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    for result in response.get('results', []):\n",
    "        results_list.append({\n",
    "            \"title\": result.get('title', 'No title'),\n",
    "            \"url\": result.get('url', 'No URL'),\n",
    "            \"snippet\": result.get('snippet') or result.get('content', 'No snippet')\n",
    "        })\n",
    "\n",
    "    #Turns result_list (a dictioanry) into a readable string.\n",
    "\n",
    "    lines = []\n",
    "    for r in enumerate(results_list, 1):\n",
    "        lines.append(\n",
    "            f\"TITLE: {r['title']}\\n\"\n",
    "            f\"URL: {r['url']}\\n\"\n",
    "            f\"Summary: {r['snippet']}\\n\"\n",
    "        )\n",
    "    response_str = \"\\n\".join(lines)\n",
    "    return str(response_str)\n",
    "\n",
    "\n",
    "\n",
    "@tool \n",
    "def specificInfo(query_specific: str) -> str:\n",
    "    \"\"\"\n",
    "    the input will be labelled query_general, which is the query that is to be sent to the ai, to find information regarding the news and researches focused on data referring to the specific stock in question. The top 10 relevant information will be recorded. \n",
    "    news within the most recent 12 months will be considered.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the API key from environment\n",
    "    API_KEY = os.environ.get(\"VALYU_API_KEY\")\n",
    "    # Initialize the Valyu client\n",
    "    valyu = Valyu(api_key=API_KEY)\n",
    "\n",
    "    # ---------- NEWS SEARCH ----------\n",
    "    news_response = valyu.search(\n",
    "        query=query_specific,\n",
    "        search_type=\"news\",\n",
    "        max_num_results=10,\n",
    "        relevance_threshold=0.7,\n",
    "        max_price=0.0, #free content only\n",
    "        start_date=datetime.now().date() - timedelta(days=365),\n",
    "        end_date=datetime.now().date(),\n",
    "        excluded_sources=[\"reddit.com\", \"twitter.com\", \"x.com\"],\n",
    "        response_length=\"medium\",\n",
    "        fast_mode=False,\n",
    "    )\n",
    "\n",
    "    # ---------- PROPRIETARY SEARCH ----------\n",
    "    proprietary_response = valyu.search(\n",
    "        query=query_specific,\n",
    "        search_type=\"proprietary\",\n",
    "        max_num_results=10,\n",
    "        relevance_threshold=0.7,\n",
    "        max_price=0.0,\n",
    "        start_date=datetime.now().date() - timedelta(days=365),\n",
    "        end_date=datetime.now().date(),\n",
    "        response_length=\"medium\",\n",
    "        fast_mode=False,\n",
    "    )\n",
    "\n",
    "    # Combine results safely\n",
    "    response = (\n",
    "        (news_response.get(\"results\") or []) +\n",
    "        (proprietary_response.get(\"results\") or [])\n",
    "    )\n",
    "\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    for result in response.get('results', []):\n",
    "        results_list.append({\n",
    "            \"title\": result.get('title', 'No title'),\n",
    "            \"url\": result.get('url', 'No URL'),\n",
    "            \"snippet\": result.get('snippet') or result.get('content', 'No snippet')\n",
    "        })\n",
    "\n",
    "    #Turns result_list (a dictioanry) into a readable string.\n",
    "\n",
    "    lines = []\n",
    "    for r in enumerate(results_list, 1):\n",
    "        lines.append(\n",
    "            f\"TITLE: {r['title']}\\n\"\n",
    "            f\"URL: {r['url']}\\n\"\n",
    "            f\"Summary: {r['snippet']}\\n\"\n",
    "        )\n",
    "    response_str = \"\\n\".join(lines)\n",
    "    return str(response_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_prompt = \"You are a Quantitative Analyst. Use the provided ML and Statistical tools to analyze the stock ticker provided. ONLY ENTER THE ABBREVIATION OF THE STOCK TO THE TOOLS. Summarize the technical outlook.\"\n",
    "trend_agent = create_agent(model, system_prompt=SystemMessage(content=[{\"type\": \"text\", \"text\": trend_prompt}, {\"type\": \"text\", \"text\": \"stock markets\"}], ), tools=[mlModel, brownianModel])\n",
    "\n",
    "noise_prompt = \"You are a Market Researcher. Use the search tool to find recent news, sentiment, and macro factors affecting the stock.\"\n",
    "noise_agent = create_agent(model, [valyu_search_tool], system_prompt=SystemMessage(content=[{\"type\": \"text\", \"text\": noise_prompt}, {\"type\": \"text\", \"text\": \"stock markets\"}], ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trend_agent.invoke({\"messages\": [HumanMessage(\"analyze AMZN stock\")]})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langgraph.types import Send\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    query: str\n",
    "    results: Annotated[List[str], operator.add] \n",
    "\n",
    "def run_trend_agent(state: AgentState):\n",
    "    \"\"\"Invokes the Quant Agent\"\"\"\n",
    "    print(\"Executing Trend Agent\")\n",
    "    response = trend_agent.invoke({\"messages\": HumanMessage(f\"Analyze {state['query']}\")})\n",
    "    return {\"results\": [f\"QUANT ANALYSIS:\\n{response['output']}\"]}\n",
    "\n",
    "def run_noise_agent(state: AgentState):\n",
    "    \"\"\"Invokes the Research Agent\"\"\"\n",
    "    print(\"Executing Noise Agent\")\n",
    "    response = noise_agent.invoke({\"messages\": HumanMessage(f\"Find news for {state['query']}\")})\n",
    "    return {\"results\": [f\"RESEARCH ANALYSIS:\\n{response['output']}\"]}\n",
    "\n",
    "def aggregator(state: AgentState):\n",
    "    \"\"\"Combines results into a final answer\"\"\"\n",
    "    print(\"Aggregating Results\")\n",
    "    final_prompt = (\n",
    "        f\"Combine the following reports into a comprehensive investment memo for {state['query']}:\\n\\n\"\n",
    "        + \"\\n\\n\".join(state[\"results\"])\n",
    "    )\n",
    "    response = model.invoke({\"messages\" : HumanMessage(final_prompt)})\n",
    "    print(f\"\\nFINAL ANSWER:\\n{response.content}\")\n",
    "    return {\"results\": [response.content]}\n",
    "\n",
    "class RouteSchema(BaseModel):\n",
    "    targets: List[Literal[\"quant\", \"research\"]] = Field(\n",
    "        description=\"Which agents to hire? Quant for numbers/charts, Research for news/sentiment.\"\n",
    "    )\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "\n",
    "workflow.add_node(\"trend_node\", run_trend_agent)\n",
    "workflow.add_node(\"noise_node\", run_noise_agent)\n",
    "workflow.add_node(\"aggregator\", aggregator)\n",
    "\n",
    "\n",
    "def route_query(state: AgentState) -> List[Send]:\n",
    "    structured_llm = model.with_structured_output(RouteSchema)\n",
    "    decision = structured_llm.invoke(f\"Analyze: {state['query']}\")\n",
    "    \n",
    "    routes = []\n",
    "    if \"quant\" in decision.targets:\n",
    "        routes.append(Send(\"trend_node\", state))\n",
    "    if \"research\" in decision.targets:\n",
    "        routes.append(Send(\"noise_node\", state))\n",
    "    if not routes:\n",
    "        routes = [Send(\"trend_node\", state), Send(\"noise_node\", state)]\n",
    "        \n",
    "    return routes\n",
    "\n",
    "workflow.add_conditional_edges(START, route_query)\n",
    "workflow.add_edge(\"trend_node\", \"aggregator\")\n",
    "workflow.add_edge(\"noise_node\", \"aggregator\")\n",
    "workflow.add_edge(\"aggregator\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_query = \"What is the outlook for AAPL stock?\"\n",
    "    inputs = {\"query\": HumanMessage(user_query), \"results\": []}\n",
    "\n",
    "    for output in app.stream(inputs):\n",
    "        pass \n"
>>>>>>> Stashed changes
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< Updated upstream
   "display_name": "Python 3",
=======
   "display_name": "base",
>>>>>>> Stashed changes
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< Updated upstream
   "version": "3.13.2"
=======
   "version": "3.13.5"
>>>>>>> Stashed changes
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
