{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force reinstall all langchain packages to the latest matching versions\n",
    "%pip install -U --force-reinstall langchain langchain-community langchain-core langchain-google-genai valyu prophet yfinance matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import operator\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Annotated, Literal, TypedDict, List\n",
    "\n",
    "# --- LIBRARIES ---\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "from langgraph.prebuilt import create_react_agent \n",
    "from pydantic import BaseModel, Field\n",
    "from valyu import Valyu \n",
    "from langchain.messages import SystemMessage, HumanMessage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=1.0,  \n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def mlModel(ticker: str):\n",
    "    \"\"\" \n",
    "    Uses Facebook Prophet to predict stock prices 30 days ahead.\n",
    "    Returns daily price targets and saves a plot to a .png file.\n",
    "    the png files name is generated by graph_filename = f\"{ticker}_forecast.png\"\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # search stock data\n",
    "\n",
    "        data = yf.Ticker(ticker).history(period=\"2y\")\n",
    "        if data.empty:\n",
    "            return \"Error: No data found\"\n",
    "        \n",
    "        # Format for prophet\n",
    "        \n",
    "        df = data.reset_index()\n",
    "        df['ds'] = df['Date'].dt.tz_localize(None)\n",
    "        df['y'] = df['Close']\n",
    "\n",
    "        # Train\n",
    "\n",
    "        m = Prophet(daily_seasonality=True)\n",
    "        m.fit(df)\n",
    "\n",
    "        # Predict\n",
    "\n",
    "        future = m.make_future_dataframe(periods=30)\n",
    "        forecast = m.predict(future)\n",
    "\n",
    "        # Save the Graph\n",
    "\n",
    "        fig1 = m.plot(forecast)\n",
    "        plt.title(f\"{ticker} 30-Day Forecast\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Price\")\n",
    "\n",
    "        graph_filename = f\"{ticker}_forecast.png\"\n",
    "        plt.savefig(graph_filename)\n",
    "        plt.close()\n",
    "\n",
    "        # List 30 day prediction\n",
    "        future_data = forecast.tail(30)\n",
    "\n",
    "        daily_tracking = []\n",
    "        for index, row in future_data.iterrows():\n",
    "            date_str = row['ds'].strftime('%Y-%m-%d')\n",
    "            price_str = f\"${row['yhat']:.2f}\"\n",
    "            daily_tracking.append(f\"{date_str}: {price_str}\")\n",
    "\n",
    "        daily_summary = \"\\n\".join(daily_tracking)\n",
    "\n",
    "        latest_pred = forecast.iloc[-1]['yhat']\n",
    "        current_price = df.iloc[-1]['y']\n",
    "        trend = \"UP\" if latest_pred > current_price else \"DOWN\"\n",
    "        print(trend)\n",
    "        \n",
    "        # Return Everything\n",
    "        return (f\"Analysis Complete for {ticker}\\n\"\n",
    "                f\"Graph saved to: {graph_filename}\\n\"\n",
    "                f\"Trend: {trend}\\n\\n\"\n",
    "                f\"Daily Price Targets (Next 30 Days):\\n\"\n",
    "                f\"{daily_summary}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Prediction failed: {e}\"\n",
    "    \n",
    "\n",
    "@tool \n",
    "def brownianModel(TICKER: str):\n",
    "    \"\"\" \n",
    "    Uses geometric brownian motion and monte carlo method to predict stock prices 30 days ahead, for a given ticker. \n",
    "    Returns daily price predictions 30 trading days ahead, with a historical lookback period of 24 months.\n",
    "    This can only be used if the stock has at least a two-year old history. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        # main variables\n",
    "        scen_size = 1000 # Reduced for speed, increase if needed\n",
    "        HISTORICAL_YEARS = 2\n",
    "        PREDICTION_DAYS = 30\n",
    "        stock_name = TICKER\n",
    "\n",
    "        end_date = pd.Timestamp.today().normalize()\n",
    "        start_date = end_date - pd.DateOffset(years=HISTORICAL_YEARS)\n",
    "        pred_end_date = end_date + pd.tseries.offsets.BDay(PREDICTION_DAYS)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Download and prepare data\n",
    "        # -----------------------------\n",
    "        # Fix for yfinance returning multi-index\n",
    "        prices = yf.download(tickers=stock_name, start=start_date, end=pred_end_date, progress=False)\n",
    "        \n",
    "        if prices.empty:\n",
    "            return f\"Error: No data found for {stock_name}\"\n",
    "\n",
    "        # Handle yfinance MultiIndex (recent API change)\n",
    "        if isinstance(prices.columns, pd.MultiIndex):\n",
    "            prices = prices['Close']\n",
    "            # If still a dataframe (multiple tickers?), select the specific ticker\n",
    "            if isinstance(prices, pd.DataFrame) and stock_name in prices.columns:\n",
    "                 prices = prices[stock_name]\n",
    "        elif 'Close' in prices.columns:\n",
    "            prices = prices['Close']\n",
    "        \n",
    "        # Ensure we have a Series, not a DataFrame\n",
    "        if isinstance(prices, pd.DataFrame):\n",
    "             prices = prices.iloc[:, 0]\n",
    "\n",
    "        # Generate business days (weekdays only)\n",
    "        future_dates = pd.bdate_range(start=pd.to_datetime(end_date) + pd.Timedelta(days=1),\n",
    "                        end=pd.to_datetime(pred_end_date))\n",
    "\n",
    "        train_set = prices.loc[:end_date]\n",
    "        \n",
    "        if len(train_set) < 2:\n",
    "            return \"Error: Not enough historical data for prediction.\"\n",
    "\n",
    "        daily_returns = ((train_set / train_set.shift(1)) - 1).dropna()\n",
    "\n",
    "        So = train_set.iloc[-1]\n",
    "        dt = 1  # day\n",
    "        \n",
    "        # Calculate volatility and drift\n",
    "        mu = np.mean(daily_returns)\n",
    "        sigma = np.std(daily_returns)\n",
    "        \n",
    "        # If sigma is 0 or NaN, we can't predict\n",
    "        if np.isnan(sigma) or sigma == 0:\n",
    "            return \"Error: Volatility calculation failed (sigma is 0 or NaN).\"\n",
    "\n",
    "        # Simulation\n",
    "        T_days = len(future_dates)\n",
    "        N = T_days\n",
    "        t = np.arange(1, N + 1)\n",
    "\n",
    "        b = {str(scen): np.random.normal(0, 1, N) for scen in range(1, scen_size + 1)}\n",
    "        W = {str(scen): b[str(scen)].cumsum() for scen in range(1, scen_size + 1)}\n",
    "\n",
    "        drift = (mu - 0.5 * sigma ** 2) * t\n",
    "        diffusion = {str(scen): sigma * W[str(scen)] for scen in range(1, scen_size + 1)}\n",
    "\n",
    "        S = np.array([So * np.exp(drift + diffusion[str(scen)]) for scen in range(1, scen_size + 1)])\n",
    "        \n",
    "        # Average prediction\n",
    "        S_pred = np.mean(S, axis=0)\n",
    "\n",
    "        final_df = pd.DataFrame({\n",
    "            'pred': S_pred\n",
    "        }, index=future_dates[:len(S_pred)])\n",
    "\n",
    "        # Create output string\n",
    "        rows = [f\"{date.date()}: ${price:.2f}\" for date, price in zip(final_df.index, final_df['pred'])]\n",
    "        result = '\\n'.join(rows)\n",
    "        print(result)\n",
    "\n",
    "        if not result:\n",
    "            return \"Error: Model ran but produced no output rows.\"\n",
    "\n",
    "        return f\"Brownian Motion Forecast for {stock_name}:\\n{result}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        # This prevents the 'ValueError: contents are required' crash\n",
    "        return f\"Brownian Model failed: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool \n",
    "#query_general is the prompt given to Valyu, and it should be on general / industry specific news/articles instead of stoc specific.\n",
    "#max_results refers to the number of sources that should be returned by the function. (Top N)\n",
    "def generalInfo(query_general: str) -> str:\n",
    "    \"\"\"\n",
    "    query_general: the query that is to be sent to the ai, to find information regarding the news and researches focused on the macroeconomy and broad industry-specific news and researches relating to the stock. The top 10 relevant information will be recorded. \n",
    "    news within the most recent 12 months will be considered.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the API key from environment\n",
    "    API_KEY = os.environ.get(\"VALYU_API_KEY\")\n",
    "    # Initialize the Valyu client\n",
    "    valyu = Valyu(api_key=API_KEY)\n",
    "\n",
    "    # ---------- NEWS SEARCH ----------\n",
    "    news_response = valyu.search(\n",
    "        query=query_general,\n",
    "        search_type=\"news\",\n",
    "        max_num_results=10,\n",
    "        relevance_threshold=0.7,\n",
    "        max_price=0.0, #free content only\n",
    "        start_date=datetime.now().date() - timedelta(days=365),\n",
    "        end_date=datetime.now().date(),\n",
    "        excluded_sources=[\"reddit.com\", \"twitter.com\", \"x.com\"],\n",
    "        response_length=\"medium\",\n",
    "        fast_mode=False,\n",
    "    )\n",
    "\n",
    "    # ---------- PROPRIETARY SEARCH ----------\n",
    "    proprietary_response = valyu.search(\n",
    "        query=query_general,\n",
    "        search_type=\"proprietary\",\n",
    "        max_num_results=10,\n",
    "        relevance_threshold=0.7,\n",
    "        max_price=0.0,\n",
    "        start_date=datetime.now().date() - timedelta(days=365),\n",
    "        end_date=datetime.now().date(),\n",
    "        response_length=\"medium\",\n",
    "        fast_mode=False,\n",
    "    )\n",
    "\n",
    "    # Combine results safely\n",
    "    response = (\n",
    "        (news_response.get(\"results\") or []) +\n",
    "        (proprietary_response.get(\"results\") or [])\n",
    "    )\n",
    "\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    for result in response.get('results', []):\n",
    "        results_list.append({\n",
    "            \"title\": result.get('title', 'No title'),\n",
    "            \"url\": result.get('url', 'No URL'),\n",
    "            \"snippet\": result.get('snippet') or result.get('content', 'No snippet')\n",
    "        })\n",
    "\n",
    "    #Turns result_list (a dictioanry) into a readable string.\n",
    "\n",
    "    lines = []\n",
    "    for r in enumerate(results_list, 1):\n",
    "        lines.append(\n",
    "            f\"TITLE: {r['title']}\\n\"\n",
    "            f\"URL: {r['url']}\\n\"\n",
    "            f\"Summary: {r['snippet']}\\n\"\n",
    "        )\n",
    "    response_str = \"\\n\".join(lines)\n",
    "    return str(response_str)\n",
    "\n",
    "\n",
    "\n",
    "@tool \n",
    "def specificInfo(query_specific: str) -> str:\n",
    "    \"\"\"\n",
    "    the input will be labelled query_general, which is the query that is to be sent to the ai, to find information regarding the news and researches focused on data referring to the specific stock in question. The top 10 relevant information will be recorded. \n",
    "    news within the most recent 12 months will be considered.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the API key from environment\n",
    "    API_KEY = os.environ.get(\"VALYU_API_KEY\")\n",
    "    # Initialize the Valyu client\n",
    "    valyu = Valyu(api_key=API_KEY)\n",
    "\n",
    "    # ---------- NEWS SEARCH ----------\n",
    "    news_response = valyu.search(\n",
    "        query=query_specific,\n",
    "        search_type=\"news\",\n",
    "        max_num_results=10,\n",
    "        relevance_threshold=0.7,\n",
    "        max_price=0.0, #free content only\n",
    "        start_date=datetime.now().date() - timedelta(days=365),\n",
    "        end_date=datetime.now().date(),\n",
    "        excluded_sources=[\"reddit.com\", \"twitter.com\", \"x.com\"],\n",
    "        response_length=\"medium\",\n",
    "        fast_mode=False,\n",
    "    )\n",
    "\n",
    "    # ---------- PROPRIETARY SEARCH ----------\n",
    "    proprietary_response = valyu.search(\n",
    "        query=query_specific,\n",
    "        search_type=\"proprietary\",\n",
    "        max_num_results=10,\n",
    "        relevance_threshold=0.7,\n",
    "        max_price=0.0,\n",
    "        start_date=datetime.now().date() - timedelta(days=365),\n",
    "        end_date=datetime.now().date(),\n",
    "        response_length=\"medium\",\n",
    "        fast_mode=False,\n",
    "    )\n",
    "\n",
    "    # Combine results safely\n",
    "    response = (\n",
    "        (news_response.get(\"results\") or []) +\n",
    "        (proprietary_response.get(\"results\") or [])\n",
    "    )\n",
    "\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    for result in response.get('results', []):\n",
    "        results_list.append({\n",
    "            \"title\": result.get('title', 'No title'),\n",
    "            \"url\": result.get('url', 'No URL'),\n",
    "            \"snippet\": result.get('snippet') or result.get('content', 'No snippet')\n",
    "        })\n",
    "\n",
    "    #Turns result_list (a dictioanry) into a readable string.\n",
    "\n",
    "    lines = []\n",
    "    for r in enumerate(results_list, 1):\n",
    "        lines.append(\n",
    "            f\"TITLE: {r['title']}\\n\"\n",
    "            f\"URL: {r['url']}\\n\"\n",
    "            f\"Summary: {r['snippet']}\\n\"\n",
    "        )\n",
    "    response_str = \"\\n\".join(lines)\n",
    "    return str(response_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_prompt = \"You are a Quantitative Analyst. Use the provided ML and Statistical tools to analyze the stock ticker provided. ONLY ENTER THE ABBREVIATION OF THE STOCK TO THE TOOLS. Summarize the technical outlook.\"\n",
    "trend_agent = create_agent(model, system_prompt=SystemMessage(content=[{\"type\": \"text\", \"text\": trend_prompt}, {\"type\": \"text\", \"text\": \"stock markets\"}], ), tools=[mlModel, brownianModel])\n",
    "\n",
    "noise_prompt = \"You are a Market Researcher. Use the search tool to find recent news, sentiment, and macro factors affecting the stock.\"\n",
    "noise_agent = create_agent(model, [valyu_search_tool], system_prompt=SystemMessage(content=[{\"type\": \"text\", \"text\": noise_prompt}, {\"type\": \"text\", \"text\": \"stock markets\"}], ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trend_agent.invoke({\"messages\": [HumanMessage(\"analyze AMZN stock\")]})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langgraph.types import Send\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    query: str\n",
    "    results: Annotated[List[str], operator.add] \n",
    "\n",
    "def run_trend_agent(state: AgentState):\n",
    "    \"\"\"Invokes the Quant Agent\"\"\"\n",
    "    print(\"Executing Trend Agent\")\n",
    "    response = trend_agent.invoke({\"messages\": HumanMessage(f\"Analyze {state['query']}\")})\n",
    "    return {\"results\": [f\"QUANT ANALYSIS:\\n{response['output']}\"]}\n",
    "\n",
    "def run_noise_agent(state: AgentState):\n",
    "    \"\"\"Invokes the Research Agent\"\"\"\n",
    "    print(\"Executing Noise Agent\")\n",
    "    response = noise_agent.invoke({\"messages\": HumanMessage(f\"Find news for {state['query']}\")})\n",
    "    return {\"results\": [f\"RESEARCH ANALYSIS:\\n{response['output']}\"]}\n",
    "\n",
    "def aggregator(state: AgentState):\n",
    "    \"\"\"Combines results into a final answer\"\"\"\n",
    "    print(\"Aggregating Results\")\n",
    "    final_prompt = (\n",
    "        f\"Combine the following reports into a comprehensive investment memo for {state['query']}:\\n\\n\"\n",
    "        + \"\\n\\n\".join(state[\"results\"])\n",
    "    )\n",
    "    response = model.invoke({\"messages\" : HumanMessage(final_prompt)})\n",
    "    print(f\"\\nFINAL ANSWER:\\n{response.content}\")\n",
    "    return {\"results\": [response.content]}\n",
    "\n",
    "class RouteSchema(BaseModel):\n",
    "    targets: List[Literal[\"quant\", \"research\"]] = Field(\n",
    "        description=\"Which agents to hire? Quant for numbers/charts, Research for news/sentiment.\"\n",
    "    )\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "\n",
    "workflow.add_node(\"trend_node\", run_trend_agent)\n",
    "workflow.add_node(\"noise_node\", run_noise_agent)\n",
    "workflow.add_node(\"aggregator\", aggregator)\n",
    "\n",
    "\n",
    "def route_query(state: AgentState) -> List[Send]:\n",
    "    structured_llm = model.with_structured_output(RouteSchema)\n",
    "    decision = structured_llm.invoke(f\"Analyze: {state['query']}\")\n",
    "    \n",
    "    routes = []\n",
    "    if \"quant\" in decision.targets:\n",
    "        routes.append(Send(\"trend_node\", state))\n",
    "    if \"research\" in decision.targets:\n",
    "        routes.append(Send(\"noise_node\", state))\n",
    "    if not routes:\n",
    "        routes = [Send(\"trend_node\", state), Send(\"noise_node\", state)]\n",
    "        \n",
    "    return routes\n",
    "\n",
    "workflow.add_conditional_edges(START, route_query)\n",
    "workflow.add_edge(\"trend_node\", \"aggregator\")\n",
    "workflow.add_edge(\"noise_node\", \"aggregator\")\n",
    "workflow.add_edge(\"aggregator\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_query = \"What is the outlook for AAPL stock?\"\n",
    "    inputs = {\"query\": HumanMessage(user_query), \"results\": []}\n",
    "\n",
    "    for output in app.stream(inputs):\n",
    "        pass \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
